{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lunarlander_ddpgv1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1zYaySbummw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk2JI2aMf32z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.float\n",
        "\n",
        "def matrix_to_tensor(matrix, device=device, dtype=dtype):\n",
        "    tensor = torch.tensor(matrix, dtype=dtype).to(device)\n",
        "    return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ors9pcPG0LcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OrnstienUhlenbeckNoise(object):\n",
        "    def __init__(self, mu, dt=1e-2, theta=0.15, sigma=0.2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.mu = mu\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.xprev + self.theta * (self.mu - self.xprev) * self.dt + \\\n",
        "        self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.xprev  = x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.xprev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT5qJwi00tW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, buffer_size, batch_size, input_dims, num_actions):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_count = 0\n",
        "        self.states_memory = np.zeros((buffer_size, *input_dims))\n",
        "        self.nextstates_memory = np.zeros((buffer_size, *input_dims))\n",
        "        self.actions_memory = np.zeros((buffer_size, num_actions))\n",
        "        self.rewards_memory = np.zeros((buffer_size))\n",
        "        self.dones_memory = np.zeros((buffer_size))\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        idx = self.buffer_count % self.buffer_size\n",
        "\n",
        "        self.states_memory[idx] = state\n",
        "        self.actions_memory[idx] = action\n",
        "        self.rewards_memory[idx] = reward\n",
        "        self.nextstates_memory[idx] = state_\n",
        "        self.dones_memory[idx] = 1 - int(done)\n",
        "\n",
        "        self.buffer_count += 1\n",
        "\n",
        "    def sample_memory(self):\n",
        "        max_mem = min(self.buffer_count, self.buffer_size)\n",
        "        idxs = np.random.choice(max_mem, self.batch_size)\n",
        "\n",
        "        states = self.states_memory[idxs]\n",
        "        actions = self.actions_memory[idxs]\n",
        "        rewards = self.rewards_memory[idxs]\n",
        "        nextstates = self.nextstates_memory[idxs]\n",
        "        dones = self.dones_memory[idxs]\n",
        "\n",
        "        return states, actions, rewards, nextstates, dones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mXJgj1KuLrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, num_actions, beta, name, batch_size=64,\n",
        "                 chkpt_dir='drive/My Drive/ddpg/'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.beta = beta\n",
        "        self.input_dims = input_dims\n",
        "        self.num_actions = num_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        self.model_file = self.chkpt_dir+name+'_llddpg'\n",
        "\n",
        "        self.num_l1 = 400\n",
        "        self.num_l2 = 300\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.num_l1)\n",
        "        f1 = 1. / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "        nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "        self.bn1 = nn.LayerNorm(self.num_l1)\n",
        "\n",
        "        self.fc2_val1 = nn.Linear(self.num_l1, self.num_l2)\n",
        "        f2 = 1. / np.sqrt(self.fc2_val1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc2_val1.weight.data, -f2, f2)\n",
        "        self.bn2 = nn.LayerNorm(self.num_l2)\n",
        "\n",
        "        self.fc2_val2 = nn.Linear(self.num_actions, self.num_l2)\n",
        "\n",
        "        self.qval = nn.Linear(self.num_l2, 1)\n",
        "        f3 = 0.003\n",
        "        nn.init.uniform_(self.qval.weight.data, -f3, f3)\n",
        "        nn.init.uniform_(self.qval.bias.data, -f3, f3)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=self.beta)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        layer = self.fc1(state)\n",
        "        layer = self.bn1(layer)\n",
        "        layer = F.relu(layer)\n",
        "\n",
        "        layer = self.fc2_val1(layer)\n",
        "        value1 = self.bn2(layer)\n",
        "        value2 = self.fc2_val2(action)\n",
        "\n",
        "        sumval = F.relu(torch.add(value1, value2))\n",
        "        qval = self.qval(sumval)\n",
        "\n",
        "        return qval\n",
        "\n",
        "    def save_model(self):\n",
        "        print('... saving model ...')\n",
        "        torch.save(self.state_dict(), self.model_file)\n",
        "\n",
        "    def load_model(self):\n",
        "        print('... loading model ...')\n",
        "        print(self.model_file)\n",
        "        if os.path.exists(self.model_file):\n",
        "            self.load_state_dict(torch.load(self.model_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g88lVONbhVqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, num_actions, alpha, action_bound, name,\n",
        "                 batch_size=64, chkpt_dir='drive/My Drive/ddpg/'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.input_dims = input_dims\n",
        "        self.num_actions = num_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        self.model_file = self.chkpt_dir+name+'_llddpg'\n",
        "        self.action_bound = action_bound\n",
        "\n",
        "        self.num_l1 = 400\n",
        "        self.num_l2 = 300\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.num_l1)\n",
        "        f1 = 1. / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "        nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "        self.bn1 = nn.LayerNorm(self.num_l1)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.num_l1, self.num_l2)\n",
        "        f2 = 1. / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
        "        self.bn2 = nn.LayerNorm(self.num_l2)\n",
        "\n",
        "        self.mu = nn.Linear(self.num_l2, self.num_actions)\n",
        "        f3 = 0.003\n",
        "        nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
        "        nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=self.alpha)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        layer = self.fc1(state)\n",
        "        layer = self.bn1(layer)\n",
        "        layer = F.relu(layer)\n",
        "        layer = self.fc2(layer)\n",
        "        layer = self.bn2(layer)\n",
        "        layer = F.relu(layer)\n",
        "        layer = self.mu(layer)\n",
        "        mu = torch.tanh(layer)\n",
        "\n",
        "        return mu\n",
        "\n",
        "    def save_model(self):\n",
        "        print('... saving model ...')\n",
        "        torch.save(self.state_dict(), self.model_file)\n",
        "\n",
        "    def load_model(self):\n",
        "        print('... loading model ...')\n",
        "        print(self.model_file)\n",
        "        if os.path.exists(self.model_file):\n",
        "            self.load_state_dict(torch.load(self.model_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_t_9KMgyIQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetworkField(object):\n",
        "    def __init__(self, env, input_dims, num_actions, batch_size=64,\n",
        "                 alpha=0.00005, beta=0.0005, gamma=0.99, tau=0.001):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.critic_eval = CriticNetwork(input_dims, num_actions, beta,\n",
        "                           'critic_eval')\n",
        "        self.actor_eval = ActorNetwork(input_dims, num_actions, alpha,\n",
        "                          env.action_space.high, 'actor_eval')\n",
        "\n",
        "        self.critic_target = CriticNetwork(input_dims, num_actions, beta,\n",
        "                            'critic_target')\n",
        "        self.actor_target = ActorNetwork(input_dims, num_actions, alpha,\n",
        "                          env.action_space.high, 'actor_target')\n",
        "\n",
        "        self.noise = OrnstienUhlenbeckNoise(np.zeros(num_actions))\n",
        "        self.replay_buffer = ReplayBuffer(1000000, batch_size, input_dims,\n",
        "                                        num_actions)\n",
        "\n",
        "        self.critic_target.load_state_dict(\n",
        "                                dict(self.critic_eval.named_parameters()))\n",
        "        self.actor_target.load_state_dict(\n",
        "                                dict(self.actor_eval.named_parameters()))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state = matrix_to_tensor(state[np.newaxis, :])\n",
        "        self.actor_eval.eval()\n",
        "        mu = self.actor_eval.forward(state) + matrix_to_tensor(self.noise())\n",
        "        mu *= matrix_to_tensor(self.actor_eval.action_bound)\n",
        "        self.actor_eval.train()\n",
        "\n",
        "        return mu.cpu().detach().numpy()[0]\n",
        "\n",
        "    def make_action(self, state):\n",
        "        state = matrix_to_tensor(state[np.newaxis, :])\n",
        "        self.actor_eval.eval()\n",
        "        mu = self.actor_eval.forward(state)\n",
        "        mu *= matrix_to_tensor(self.actor_eval.action_bound)\n",
        "\n",
        "        return mu.cpu().detach().numpy()[0]\n",
        "\n",
        "    def store_experience(self, state, action, reward, state_, done):\n",
        "        self.replay_buffer.store_transition(state, action, reward, state_, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if self.replay_buffer.buffer_count < self.batch_size:\n",
        "            return False\n",
        "\n",
        "        states, actions, rewards, nextstates, dones = \\\n",
        "                                self.replay_buffer.sample_memory()\n",
        "\n",
        "        states = matrix_to_tensor(states)\n",
        "        actions = matrix_to_tensor(actions)\n",
        "        nextstates = matrix_to_tensor(nextstates)\n",
        "        rewards = matrix_to_tensor(rewards)\n",
        "        dones = matrix_to_tensor(dones)\n",
        "\n",
        "        self.critic_target.eval()\n",
        "        self.actor_target.eval()\n",
        "        self.critic_eval.eval()\n",
        "\n",
        "        nextactions = self.actor_target.forward(nextstates)\n",
        "        nextqvals = self.critic_target.forward(nextstates, nextactions)\n",
        "        qvals = self.critic_eval.forward(states, actions)\n",
        "\n",
        "        yvals = []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            yvals.append(rewards[i] + self.gamma*nextqvals[i]*dones[i])\n",
        "\n",
        "        yvals = matrix_to_tensor(yvals)\n",
        "        yvals = yvals.view(self.batch_size, 1)\n",
        "\n",
        "        #critic_loss: where we optimize critic_eval by calculating yvals\n",
        "        self.critic_eval.train()\n",
        "        self.critic_eval.optimizer.zero_grad()\n",
        "        critic_loss = F.mse_loss(yvals, qvals)\n",
        "        critic_loss.backward()\n",
        "        self.critic_eval.optimizer.step()\n",
        "\n",
        "        #actor_loss: where we optimize actor_eval by calculating mus\n",
        "        self.critic_eval.eval()\n",
        "        self.actor_eval.optimizer.zero_grad()\n",
        "        mus = self.actor_eval.forward(states)\n",
        "        self.actor_eval.train()\n",
        "        actor_loss = torch.mean(-self.critic_eval(states, mus))\n",
        "        actor_loss.backward()\n",
        "        self.actor_eval.optimizer.step()\n",
        "\n",
        "        self.soft_target_update()\n",
        "\n",
        "    def soft_target_update(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        critic_target_weights = dict(self.critic_target.named_parameters())\n",
        "        actor_target_weights = dict(self.actor_target.named_parameters())\n",
        "        critic_eval_weights = dict(self.critic_eval.named_parameters())\n",
        "        actor_eval_weights = dict(self.actor_eval.named_parameters())\n",
        "\n",
        "        for name in critic_eval_weights:\n",
        "            critic_target_weights[name] = tau*critic_eval_weights[name].clone() + \\\n",
        "            (1-tau)*critic_target_weights[name].clone()\n",
        "\n",
        "        self.critic_target.load_state_dict(critic_target_weights)\n",
        "\n",
        "        for name in actor_eval_weights:\n",
        "            actor_target_weights[name] = tau*actor_eval_weights[name].clone() + \\\n",
        "            (1-tau)*actor_target_weights[name].clone()\n",
        "\n",
        "        self.actor_target.load_state_dict(actor_target_weights)\n",
        "\n",
        "    def save_field(self):\n",
        "        self.critic_target.save_model()\n",
        "        self.critic_eval.save_model()\n",
        "        self.actor_target.save_model()\n",
        "        self.actor_eval.save_model()\n",
        "\n",
        "    def load_field(self):\n",
        "        self.critic_target.load_model()\n",
        "        self.critic_eval.load_model()\n",
        "        self.actor_target.load_model()\n",
        "        self.actor_eval.load_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxefSAdkoFDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "00e40e8b-4532-4e51-b056-fa815de27106"
      },
      "source": [
        "%%bash\n",
        "pip install gym[box2d]\n",
        "apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "apt-get install x11-utils\n",
        "pip install gym pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.17.5)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils\n",
            "0 upgraded, 2 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 209 kB of archives.\n",
            "After this operation, 711 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Fetched 209 kB in 2s (110 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 147475 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\r\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Selecting previously unselected package x11-utils.\r\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\r\n",
            "Unpacking x11-utils (7.7+3build1) ...\r\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Setting up x11-utils (7.7+3build1) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.10)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/32/8f/88d636f1da22a3c573259e44cfefb46a117d3f9432e2c98b1ab4a21372ad/EasyProcess-0.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.2.10 pyvirtualdisplay-0.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1zmldzSFq44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_models(network_field):\n",
        "        if os.path.exists(network_field.actor_eval.model_file):\n",
        "            network_field.load_field()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3dfN890Nnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_lunarlander_field(env, network_field):\n",
        "    np.random.seed(0)\n",
        "\n",
        "    num_episodes = 1000\n",
        "    save_episodes = 25\n",
        "    done = False\n",
        "    score_hist = []\n",
        "    score = 0\n",
        "\n",
        "    for e in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        while not done:\n",
        "            action = network_field.choose_action(state)\n",
        "            state_, reward, done, _ = env.step(action)\n",
        "            network_field.store_experience(state, action, reward, state_, done)\n",
        "            network_field.learn()\n",
        "            score += reward\n",
        "            state = state_\n",
        "\n",
        "        if e % save_episodes == 0:\n",
        "            network_field.save_field()\n",
        "        \n",
        "        score_hist.append(score)\n",
        "        running_avg = np.mean(score_hist[-100:])\n",
        "        print('episode: ', e,\n",
        "              ' score: %.2f' % score,\n",
        "              ' running avg (last 100): %.3f' % running_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZgxEQXx4fi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_lunarlander_field(env, network_field):\n",
        "\n",
        "    num_episodes = 100\n",
        "    done = False\n",
        "    score_hist = []\n",
        "    score = 0\n",
        "\n",
        "    for e in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        frame = 0\n",
        "\n",
        "        while not done:\n",
        "            action = network_field.make_action(state)\n",
        "            state_, reward, done, _ = env.step(action)\n",
        "            score += reward\n",
        "            state = state_\n",
        "            frame += 1\n",
        "\n",
        "        score_hist.append(score)\n",
        "        running_avg = np.mean(score_hist)\n",
        "        print('episode: ', e,\n",
        "              ' score: %.2f' % score,\n",
        "              ' running avg (last 100): %.3f' % running_avg,\n",
        "              ' num frames: %d' % frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAw_x61G0YsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5d32707a-fc0d-4e44-957c-b9a2a29c233e"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLanderContinuous-v2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtZ4t7etBSWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network_field = NetworkField(env=env, input_dims=[8],\n",
        "                             num_actions=env.action_space.shape[0],\n",
        "                             batch_size=64,\n",
        "                             alpha=0.000025,\n",
        "                             beta=0.00025)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ChpmG4-sDl4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a8accacd-dac6-4638-e221-849f05c7be05"
      },
      "source": [
        "load_models(network_field)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... loading model ...\n",
            "drive/My Drive/ddpg/critic_target_llddpg\n",
            "... loading model ...\n",
            "drive/My Drive/ddpg/critic_eval_llddpg\n",
            "... loading model ...\n",
            "drive/My Drive/ddpg/actor_target_llddpg\n",
            "... loading model ...\n",
            "drive/My Drive/ddpg/actor_eval_llddpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBgYpGDX58pA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3da6ca7-b919-4064-fbec-ab8014d8aee5"
      },
      "source": [
        "test_lunarlander_field(env=env, network_field=network_field)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode:  0  score: 254.74  running avg (last 100): 254.735  num frames: 189\n",
            "episode:  1  score: 280.78  running avg (last 100): 267.757  num frames: 288\n",
            "episode:  2  score: 62.33  running avg (last 100): 199.283  num frames: 141\n",
            "episode:  3  score: 261.34  running avg (last 100): 214.797  num frames: 180\n",
            "episode:  4  score: 274.57  running avg (last 100): 226.752  num frames: 182\n",
            "episode:  5  score: 253.20  running avg (last 100): 231.160  num frames: 337\n",
            "episode:  6  score: 244.09  running avg (last 100): 233.007  num frames: 197\n",
            "episode:  7  score: 205.87  running avg (last 100): 229.616  num frames: 270\n",
            "episode:  8  score: 261.49  running avg (last 100): 233.157  num frames: 180\n",
            "episode:  9  score: 232.75  running avg (last 100): 233.116  num frames: 190\n",
            "episode:  10  score: 261.98  running avg (last 100): 235.740  num frames: 205\n",
            "episode:  11  score: 254.11  running avg (last 100): 237.271  num frames: 328\n",
            "episode:  12  score: 245.94  running avg (last 100): 237.938  num frames: 260\n",
            "episode:  13  score: 293.13  running avg (last 100): 241.880  num frames: 297\n",
            "episode:  14  score: 266.03  running avg (last 100): 243.490  num frames: 207\n",
            "episode:  15  score: 71.94  running avg (last 100): 232.768  num frames: 168\n",
            "episode:  16  score: 212.04  running avg (last 100): 231.549  num frames: 316\n",
            "episode:  17  score: 264.73  running avg (last 100): 233.392  num frames: 747\n",
            "episode:  18  score: 265.79  running avg (last 100): 235.098  num frames: 234\n",
            "episode:  19  score: 272.41  running avg (last 100): 236.963  num frames: 215\n",
            "episode:  20  score: 276.07  running avg (last 100): 238.825  num frames: 265\n",
            "episode:  21  score: 234.99  running avg (last 100): 238.651  num frames: 458\n",
            "episode:  22  score: 271.03  running avg (last 100): 240.059  num frames: 320\n",
            "episode:  23  score: 18.37  running avg (last 100): 230.822  num frames: 155\n",
            "episode:  24  score: 250.90  running avg (last 100): 231.625  num frames: 463\n",
            "episode:  25  score: 56.04  running avg (last 100): 224.872  num frames: 147\n",
            "episode:  26  score: 255.24  running avg (last 100): 225.997  num frames: 277\n",
            "episode:  27  score: 288.51  running avg (last 100): 228.230  num frames: 390\n",
            "episode:  28  score: 66.14  running avg (last 100): 222.640  num frames: 150\n",
            "episode:  29  score: 265.14  running avg (last 100): 224.057  num frames: 209\n",
            "episode:  30  score: 269.20  running avg (last 100): 225.514  num frames: 206\n",
            "episode:  31  score: 274.74  running avg (last 100): 227.052  num frames: 444\n",
            "episode:  32  score: 267.99  running avg (last 100): 228.293  num frames: 461\n",
            "episode:  33  score: 299.77  running avg (last 100): 230.395  num frames: 234\n",
            "episode:  34  score: 256.07  running avg (last 100): 231.128  num frames: 208\n",
            "episode:  35  score: 213.56  running avg (last 100): 230.640  num frames: 511\n",
            "episode:  36  score: 284.73  running avg (last 100): 232.102  num frames: 237\n",
            "episode:  37  score: 234.54  running avg (last 100): 232.167  num frames: 234\n",
            "episode:  38  score: 263.30  running avg (last 100): 232.965  num frames: 277\n",
            "episode:  39  score: 73.91  running avg (last 100): 228.989  num frames: 212\n",
            "episode:  40  score: 218.86  running avg (last 100): 228.742  num frames: 615\n",
            "episode:  41  score: 280.66  running avg (last 100): 229.978  num frames: 206\n",
            "episode:  42  score: 252.72  running avg (last 100): 230.506  num frames: 200\n",
            "episode:  43  score: 208.22  running avg (last 100): 230.000  num frames: 421\n",
            "episode:  44  score: 209.37  running avg (last 100): 229.542  num frames: 421\n",
            "episode:  45  score: 279.24  running avg (last 100): 230.622  num frames: 223\n",
            "episode:  46  score: 272.89  running avg (last 100): 231.521  num frames: 206\n",
            "episode:  47  score: 185.82  running avg (last 100): 230.569  num frames: 475\n",
            "episode:  48  score: 223.68  running avg (last 100): 230.429  num frames: 498\n",
            "episode:  49  score: 251.73  running avg (last 100): 230.855  num frames: 333\n",
            "episode:  50  score: 64.25  running avg (last 100): 227.588  num frames: 180\n",
            "episode:  51  score: 284.16  running avg (last 100): 228.676  num frames: 299\n",
            "episode:  52  score: 257.30  running avg (last 100): 229.216  num frames: 191\n",
            "episode:  53  score: -182.80  running avg (last 100): 221.586  num frames: 573\n",
            "episode:  54  score: 255.45  running avg (last 100): 222.202  num frames: 734\n",
            "episode:  55  score: 243.55  running avg (last 100): 222.583  num frames: 451\n",
            "episode:  56  score: 227.59  running avg (last 100): 222.671  num frames: 237\n",
            "episode:  57  score: 242.99  running avg (last 100): 223.021  num frames: 405\n",
            "episode:  58  score: 249.35  running avg (last 100): 223.467  num frames: 359\n",
            "episode:  59  score: 288.11  running avg (last 100): 224.545  num frames: 310\n",
            "episode:  60  score: 303.65  running avg (last 100): 225.841  num frames: 262\n",
            "episode:  61  score: 282.48  running avg (last 100): 226.755  num frames: 205\n",
            "episode:  62  score: 13.53  running avg (last 100): 223.370  num frames: 152\n",
            "episode:  63  score: 247.95  running avg (last 100): 223.754  num frames: 196\n",
            "episode:  64  score: 247.42  running avg (last 100): 224.118  num frames: 189\n",
            "episode:  65  score: 270.03  running avg (last 100): 224.814  num frames: 409\n",
            "episode:  66  score: 254.90  running avg (last 100): 225.263  num frames: 230\n",
            "episode:  67  score: 257.21  running avg (last 100): 225.733  num frames: 206\n",
            "episode:  68  score: 228.71  running avg (last 100): 225.776  num frames: 280\n",
            "episode:  69  score: 255.35  running avg (last 100): 226.199  num frames: 185\n",
            "episode:  70  score: 259.42  running avg (last 100): 226.667  num frames: 186\n",
            "episode:  71  score: -47.42  running avg (last 100): 222.860  num frames: 818\n",
            "episode:  72  score: 274.46  running avg (last 100): 223.567  num frames: 246\n",
            "episode:  73  score: 233.58  running avg (last 100): 223.702  num frames: 449\n",
            "episode:  74  score: 285.02  running avg (last 100): 224.519  num frames: 282\n",
            "episode:  75  score: 272.97  running avg (last 100): 225.157  num frames: 216\n",
            "episode:  76  score: 228.16  running avg (last 100): 225.196  num frames: 228\n",
            "episode:  77  score: 240.78  running avg (last 100): 225.396  num frames: 180\n",
            "episode:  78  score: 226.65  running avg (last 100): 225.412  num frames: 327\n",
            "episode:  79  score: 299.16  running avg (last 100): 226.334  num frames: 331\n",
            "episode:  80  score: 267.36  running avg (last 100): 226.840  num frames: 256\n",
            "episode:  81  score: 249.15  running avg (last 100): 227.112  num frames: 197\n",
            "episode:  82  score: 249.62  running avg (last 100): 227.383  num frames: 321\n",
            "episode:  83  score: 260.20  running avg (last 100): 227.774  num frames: 227\n",
            "episode:  84  score: 268.25  running avg (last 100): 228.250  num frames: 241\n",
            "episode:  85  score: -0.87  running avg (last 100): 225.586  num frames: 1000\n",
            "episode:  86  score: 277.33  running avg (last 100): 226.181  num frames: 263\n",
            "episode:  87  score: 284.79  running avg (last 100): 226.847  num frames: 247\n",
            "episode:  88  score: 253.14  running avg (last 100): 227.142  num frames: 177\n",
            "episode:  89  score: 228.07  running avg (last 100): 227.153  num frames: 222\n",
            "episode:  90  score: 286.28  running avg (last 100): 227.802  num frames: 278\n",
            "episode:  91  score: 261.00  running avg (last 100): 228.163  num frames: 320\n",
            "episode:  92  score: 271.34  running avg (last 100): 228.628  num frames: 212\n",
            "episode:  93  score: 247.65  running avg (last 100): 228.830  num frames: 220\n",
            "episode:  94  score: 256.29  running avg (last 100): 229.119  num frames: 327\n",
            "episode:  95  score: 275.59  running avg (last 100): 229.603  num frames: 235\n",
            "episode:  96  score: 271.85  running avg (last 100): 230.039  num frames: 287\n",
            "episode:  97  score: 238.86  running avg (last 100): 230.129  num frames: 178\n",
            "episode:  98  score: 261.86  running avg (last 100): 230.449  num frames: 224\n",
            "episode:  99  score: 289.63  running avg (last 100): 231.041  num frames: 188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fUZ9CsJ0euU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "0d3869c4-2d76-40c8-ce64-d20b0130fbd0"
      },
      "source": [
        "train_lunarlander_field(env=env, network_field=network_field)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... saving model ...\n",
            "... saving model ...\n",
            "... saving model ...\n",
            "... saving model ...\n",
            "episode:  0  score: 255.14  running avg (last 100): 255.141\n",
            "episode:  1  score: 264.28  running avg (last 100): 259.710\n",
            "episode:  2  score: 279.75  running avg (last 100): 266.391\n",
            "episode:  3  score: 223.94  running avg (last 100): 255.779\n",
            "episode:  4  score: -106.40  running avg (last 100): 183.343\n",
            "episode:  5  score: -359.06  running avg (last 100): 92.942\n",
            "episode:  6  score: 189.20  running avg (last 100): 106.693\n",
            "episode:  7  score: -220.02  running avg (last 100): 65.854\n",
            "episode:  8  score: -68.26  running avg (last 100): 50.952\n",
            "episode:  9  score: -573.05  running avg (last 100): -11.447\n",
            "episode:  10  score: -96.66  running avg (last 100): -19.194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7db84d4a8848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_lunarlander_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-4dc372c72d6d>\u001b[0m in \u001b[0;36mtrain_lunarlander_field\u001b[0;34m(env, network_field)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mnetwork_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mnetwork_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9da23a375e4e>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mnextactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mnextqvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mqvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-38091647012b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX0OlH7jGZ1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d466e14d-09fe-4b9f-dd0a-35b7e7a98a7c"
      },
      "source": [
        "test_lunarlander_field(env=env, network_field=network_field)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode:  0  score: 265.89  running avg (last 100): 265.887  num frames: 215\n",
            "episode:  1  score: 255.94  running avg (last 100): 260.913  num frames: 374\n",
            "episode:  2  score: 259.43  running avg (last 100): 260.417  num frames: 228\n",
            "episode:  3  score: 237.90  running avg (last 100): 254.788  num frames: 962\n",
            "episode:  4  score: -16.52  running avg (last 100): 200.527  num frames: 1000\n",
            "episode:  5  score: -29.52  running avg (last 100): 162.186  num frames: 1000\n",
            "episode:  6  score: 256.32  running avg (last 100): 175.633  num frames: 191\n",
            "episode:  7  score: 278.96  running avg (last 100): 188.548  num frames: 208\n",
            "episode:  8  score: 253.61  running avg (last 100): 195.777  num frames: 223\n",
            "episode:  9  score: -39.93  running avg (last 100): 172.206  num frames: 1000\n",
            "episode:  10  score: 277.88  running avg (last 100): 181.813  num frames: 238\n",
            "episode:  11  score: 255.45  running avg (last 100): 187.949  num frames: 206\n",
            "episode:  12  score: 1.37  running avg (last 100): 173.597  num frames: 1000\n",
            "episode:  13  score: 283.81  running avg (last 100): 181.469  num frames: 187\n",
            "episode:  14  score: 246.06  running avg (last 100): 185.775  num frames: 185\n",
            "episode:  15  score: 257.43  running avg (last 100): 190.253  num frames: 235\n",
            "episode:  16  score: 270.13  running avg (last 100): 194.952  num frames: 198\n",
            "episode:  17  score: 267.95  running avg (last 100): 199.007  num frames: 225\n",
            "episode:  18  score: 54.66  running avg (last 100): 191.410  num frames: 173\n",
            "episode:  19  score: 237.66  running avg (last 100): 193.722  num frames: 191\n",
            "episode:  20  score: 6.29  running avg (last 100): 184.797  num frames: 1000\n",
            "episode:  21  score: 251.19  running avg (last 100): 187.815  num frames: 192\n",
            "episode:  22  score: 0.27  running avg (last 100): 179.661  num frames: 1000\n",
            "episode:  23  score: 263.45  running avg (last 100): 183.152  num frames: 214\n",
            "episode:  24  score: -18.64  running avg (last 100): 175.080  num frames: 1000\n",
            "episode:  25  score: 285.95  running avg (last 100): 179.344  num frames: 211\n",
            "episode:  26  score: 250.08  running avg (last 100): 181.964  num frames: 181\n",
            "episode:  27  score: 281.75  running avg (last 100): 185.528  num frames: 203\n",
            "episode:  28  score: 274.74  running avg (last 100): 188.604  num frames: 212\n",
            "episode:  29  score: 272.42  running avg (last 100): 191.398  num frames: 206\n",
            "episode:  30  score: 258.95  running avg (last 100): 193.578  num frames: 187\n",
            "episode:  31  score: 256.08  running avg (last 100): 195.531  num frames: 199\n",
            "episode:  32  score: 220.08  running avg (last 100): 196.275  num frames: 214\n",
            "episode:  33  score: 248.35  running avg (last 100): 197.806  num frames: 237\n",
            "episode:  34  score: 229.59  running avg (last 100): 198.714  num frames: 289\n",
            "episode:  35  score: 286.31  running avg (last 100): 201.148  num frames: 217\n",
            "episode:  36  score: 259.21  running avg (last 100): 202.717  num frames: 171\n",
            "episode:  37  score: 242.34  running avg (last 100): 203.760  num frames: 227\n",
            "episode:  38  score: 250.21  running avg (last 100): 204.951  num frames: 177\n",
            "episode:  39  score: 257.49  running avg (last 100): 206.264  num frames: 348\n",
            "episode:  40  score: 213.66  running avg (last 100): 206.444  num frames: 198\n",
            "episode:  41  score: 253.25  running avg (last 100): 207.559  num frames: 188\n",
            "episode:  42  score: 1.90  running avg (last 100): 202.776  num frames: 1000\n",
            "episode:  43  score: 268.21  running avg (last 100): 204.263  num frames: 471\n",
            "episode:  44  score: 250.33  running avg (last 100): 205.287  num frames: 379\n",
            "episode:  45  score: 258.29  running avg (last 100): 206.439  num frames: 322\n",
            "episode:  46  score: -61.94  running avg (last 100): 200.729  num frames: 1000\n",
            "episode:  47  score: 275.54  running avg (last 100): 202.288  num frames: 240\n",
            "episode:  48  score: 242.76  running avg (last 100): 203.114  num frames: 216\n",
            "episode:  49  score: 257.41  running avg (last 100): 204.200  num frames: 230\n",
            "episode:  50  score: 253.50  running avg (last 100): 205.166  num frames: 195\n",
            "episode:  51  score: -13.97  running avg (last 100): 200.952  num frames: 1000\n",
            "episode:  52  score: -54.32  running avg (last 100): 196.136  num frames: 1000\n",
            "episode:  53  score: -28.29  running avg (last 100): 191.980  num frames: 1000\n",
            "episode:  54  score: 235.85  running avg (last 100): 192.777  num frames: 184\n",
            "episode:  55  score: 262.82  running avg (last 100): 194.028  num frames: 213\n",
            "episode:  56  score: 246.38  running avg (last 100): 194.946  num frames: 229\n",
            "episode:  57  score: 278.28  running avg (last 100): 196.383  num frames: 223\n",
            "episode:  58  score: 19.73  running avg (last 100): 193.389  num frames: 1000\n",
            "episode:  59  score: 286.87  running avg (last 100): 194.947  num frames: 175\n",
            "episode:  60  score: 279.46  running avg (last 100): 196.332  num frames: 178\n",
            "episode:  61  score: 237.48  running avg (last 100): 196.996  num frames: 185\n",
            "episode:  62  score: -38.61  running avg (last 100): 193.256  num frames: 1000\n",
            "episode:  63  score: -9.85  running avg (last 100): 190.083  num frames: 1000\n",
            "episode:  64  score: -7.80  running avg (last 100): 187.038  num frames: 1000\n",
            "episode:  65  score: 272.33  running avg (last 100): 188.331  num frames: 233\n",
            "episode:  66  score: 251.52  running avg (last 100): 189.274  num frames: 194\n",
            "episode:  67  score: 265.08  running avg (last 100): 190.389  num frames: 188\n",
            "episode:  68  score: 254.82  running avg (last 100): 191.322  num frames: 187\n",
            "episode:  69  score: 275.81  running avg (last 100): 192.529  num frames: 264\n",
            "episode:  70  score: 262.45  running avg (last 100): 193.514  num frames: 298\n",
            "episode:  71  score: 249.83  running avg (last 100): 194.296  num frames: 202\n",
            "episode:  72  score: 264.75  running avg (last 100): 195.261  num frames: 223\n",
            "episode:  73  score: 13.47  running avg (last 100): 192.805  num frames: 1000\n",
            "episode:  74  score: 247.31  running avg (last 100): 193.531  num frames: 257\n",
            "episode:  75  score: -10.55  running avg (last 100): 190.846  num frames: 1000\n",
            "episode:  76  score: 251.77  running avg (last 100): 191.637  num frames: 458\n",
            "episode:  77  score: 7.91  running avg (last 100): 189.282  num frames: 1000\n",
            "episode:  78  score: 240.45  running avg (last 100): 189.930  num frames: 248\n",
            "episode:  79  score: -41.89  running avg (last 100): 187.032  num frames: 1000\n",
            "episode:  80  score: 258.27  running avg (last 100): 187.911  num frames: 287\n",
            "episode:  81  score: -6.34  running avg (last 100): 185.543  num frames: 1000\n",
            "episode:  82  score: 227.87  running avg (last 100): 186.053  num frames: 213\n",
            "episode:  83  score: -14.77  running avg (last 100): 183.662  num frames: 1000\n",
            "episode:  84  score: 268.72  running avg (last 100): 184.662  num frames: 184\n",
            "episode:  85  score: -12.71  running avg (last 100): 182.367  num frames: 1000\n",
            "episode:  86  score: 284.33  running avg (last 100): 183.539  num frames: 315\n",
            "episode:  87  score: 8.99  running avg (last 100): 181.556  num frames: 1000\n",
            "episode:  88  score: 264.51  running avg (last 100): 182.488  num frames: 176\n",
            "episode:  89  score: 257.98  running avg (last 100): 183.327  num frames: 341\n",
            "episode:  90  score: 12.11  running avg (last 100): 181.445  num frames: 1000\n",
            "episode:  91  score: 257.25  running avg (last 100): 182.269  num frames: 176\n",
            "episode:  92  score: 216.27  running avg (last 100): 182.635  num frames: 182\n",
            "episode:  93  score: 7.21  running avg (last 100): 180.769  num frames: 1000\n",
            "episode:  94  score: 277.09  running avg (last 100): 181.783  num frames: 228\n",
            "episode:  95  score: 249.22  running avg (last 100): 182.485  num frames: 174\n",
            "episode:  96  score: 240.76  running avg (last 100): 183.086  num frames: 766\n",
            "episode:  97  score: 277.40  running avg (last 100): 184.048  num frames: 178\n",
            "episode:  98  score: -80.02  running avg (last 100): 181.381  num frames: 1000\n",
            "episode:  99  score: 218.75  running avg (last 100): 181.754  num frames: 639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcqYxSxHpLBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a0055cb4-1ce8-4fa6-acf6-8ba312a8be8f"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(600, 600))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '600x600x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '600x600x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HmFJ-10pJSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XvZcCuUnpe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "4dc222c2-2672-4c37-f5ab-f764cda4e244"
      },
      "source": [
        "def show_env_field(env, field, step=0, info=\"\"):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    frame = 0\n",
        "    while not done:\n",
        "        plt.figure(3)\n",
        "        plt.clf()\n",
        "        action = field.make_action(state)\n",
        "        state_, reward, done, _ = env.step(action)\n",
        "        plt.imshow(env.render(mode='rgb_array'))\n",
        "        plt.axis('off')\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        state = state_\n",
        "        score += reward\n",
        "        frame += 1\n",
        "    \n",
        "    print('Score: %.2f' % score)\n",
        "    print('Num Frames: %d' % frame)\n",
        "\n",
        "show_env_field(env, network_field)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkX8Bm/iR0SuROcEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Score: 238.15\n",
            "Num Frames: 185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkX8Bm/iR0SuROcEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGy-RSFJ2TCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ff67b51-c7b4-4b02-e4ce-d6b62d44d119"
      },
      "source": [
        "os.path.exists('drive/My Drive/ddpg/critic_target_ddpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaGYYPb53opG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}