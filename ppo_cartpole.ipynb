{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyNsszhV24uJ4mAUkP9d0tLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asceznyk/notebooks/blob/main/ppo_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn21iArHKLqA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import scipy.signal as signal\n",
        "\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.distributions.categorical import Categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "dim_obs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "9V0GZj49KyU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## networks\n",
        "\n",
        "def mlp(layers, out_dim):\n",
        "    args = []\n",
        "    for u_in, u_out in layers:\n",
        "        args.append(nn.Linear(u_in, u_out))\n",
        "        args.append(nn.LeakyReLU())\n",
        "    args.append((nn.Linear(u_out, out_dim)))\n",
        "    return nn.Sequential(*args)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, n_actions, hid_layers):\n",
        "        super(Actor, self).__init__()\n",
        "        self.net = mlp(hid_layers, n_actions) \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, hid_layers):\n",
        "        super(Critic, self).__init__()\n",
        "        self.net = mlp(hid_layers, 1) \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "UttAL6QWK7Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the buffer\n",
        "\n",
        "def discount_cum(x, d): \n",
        "    return signal.lfilter([1], [1, float(-d)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self, dim_obs, gamma=0.99, lam=0.95):\n",
        "        self.obs_buf = [] \n",
        "        self.act_buf = []\n",
        "\n",
        "        self.reward_buf = []\n",
        "        self.value_buf = [] \n",
        "        self.logp_buf = []\n",
        "\n",
        "        self.adv_buf = []  \n",
        "        self.return_buf = []\n",
        "\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.t_start = 0, 0\n",
        "\n",
        "    def terminate(self, last_val=0):\n",
        "        rewards = np.array(self.reward_buf)\n",
        "        values = np.append(self.value_buf, last_val)\n",
        "        delta = rewards + self.gamma * values[1:] - values[:-1]\n",
        "        self.adv_buf = discount_cum(delta, self.gamma * self.lam)\n",
        "        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / np.std(self.adv_buf)\n",
        "        self.return_buf = discount_cum(rewards, self.gamma) \n",
        "\n",
        "    def store(self, obs, act, reward, logp, value):\n",
        "        self.obs_buf.append(obs)\n",
        "        self.act_buf.append(act)\n",
        "        self.reward_buf.append(reward)\n",
        "        self.logp_buf.append(logp)\n",
        "        self.value_buf.append(value) \n",
        "\n",
        "    def get(self):\n",
        "        obs_buf = np.array(self.obs_buf) \n",
        "        act_buf = np.array(self.act_buf)\n",
        "        adv_buf = np.array(self.adv_buf)\n",
        "        return_buf = np.array(self.return_buf)\n",
        "        logp_buf = np.array(self.logp_buf)\n",
        "\n",
        "        self.obs_buf = [] \n",
        "        self.act_buf = []\n",
        "\n",
        "        self.reward_buf = []\n",
        "        self.value_buf = [] \n",
        "        self.logp_buf = []\n",
        "        \n",
        "        self.adv_buf = []  \n",
        "        self.return_buf = []\n",
        "\n",
        "        return (\n",
        "            obs_buf,\n",
        "            act_buf,\n",
        "            adv_buf,\n",
        "            return_buf,\n",
        "            logp_buf\n",
        "        )\n"
      ],
      "metadata": {
        "id": "90Gdjs3wQYXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## helper\n",
        "\n",
        "def to_torch(*args):\n",
        "    for t in args: \n",
        "        yield torch.from_numpy(t)\n",
        "\n",
        "def to_numpy(*args):\n",
        "    for t in args:\n",
        "        yield t.detach().numpy()\n",
        "\n",
        "def sample_action(actor, obs): \n",
        "    logits = actor(obs)\n",
        "    dist = Categorical(F.softmax(logits, dim=-1))\n",
        "    return logits, dist.sample()\n",
        "\n",
        "def log_p(logits, act_buf):\n",
        "    return torch.sum(\n",
        "        F.one_hot(act_buf, n_actions) * F.log_softmax(logits, dim=-1), dim=-1\n",
        "    )"
      ],
      "metadata": {
        "id": "VL355oPCmDbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## hyperparameters of the PPO algorithm\n",
        "\n",
        "max_ep = 500\n",
        "max_steps = 1000 \n",
        "gamma = 0.99\n",
        "eps = 0.2\n",
        "lr_policy = 3e-4\n",
        "lr_value = 1e-3\n",
        "policy_iterations = 10\n",
        "value_iterations = 10\n",
        "target_kl = 0.01\n",
        "hidden_sizes = [(dim_obs, 32), (32, 32)]\n",
        "\n",
        "render = False\n"
      ],
      "metadata": {
        "id": "KXY1OFrfwj9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## inits\n",
        "\n",
        "def init_agent():    \n",
        "    actor = Actor(n_actions, hidden_sizes) \n",
        "    critic = Critic(hidden_sizes)\n",
        "    buffer = Buffer(dim_obs)\n",
        "    policy_opt = torch.optim.Adam(actor.parameters(), lr=lr_policy)\n",
        "    value_opt = torch.optim.Adam(critic.parameters(), lr=lr_value)\n",
        "    return actor, critic, buffer, policy_opt, value_opt"
      ],
      "metadata": {
        "id": "zDE-6FCSyvBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## comparing different policy loss functions\n",
        "\n",
        "def step(opt, loss): \n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "def ppo_policy(actor, opt, obs_buf, act_buf, logp_buf, adv_buf, policy_iterations=policy_iterations):\n",
        "    for _ in range(policy_iterations): \n",
        "        logits = actor(obs_buf)  \n",
        "        policy_loss = -torch.min(\n",
        "            (log_p(logits, act_buf) - logp_buf).exp() * adv_buf, \n",
        "            torch.where(adv_buf > 0, (1+eps)*adv_buf, (1-eps)*adv_buf)\n",
        "        ).mean()\n",
        "        step(opt, policy_loss) \n",
        "                \n",
        "        kl = (logp_buf - log_p(logits, act_buf)).mean() \n",
        "        if kl > 1.5 * target_kl: break \n",
        "\n",
        "def custom_policy(actor, opt, obs_buf, act_buf, logp_buf, adv_buf, policy_iterations=policy_iterations):\n",
        "    for _ in range(policy_iterations):\n",
        "        ratios = (log_p(actor(obs_buf), act_buf) - logp_buf).exp() \n",
        "        policy_loss = -torch.min(ratios * adv_buf, torch.clamp(ratios, 1+eps, 1-eps) * adv_buf).mean()\n",
        "        step(opt, policy_loss)\n"
      ],
      "metadata": {
        "id": "zni9-e5jkh3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## typical training\n",
        "\n",
        "def train_agent(policy_loss_fn):\n",
        "    ##init new agent\n",
        "    actor, critic, buffer, policy_opt, value_opt = init_agent()\n",
        "\n",
        "    actor.train()\n",
        "    critic.train()\n",
        "    \n",
        "    final_rewards = []\n",
        "    obs, ep_ret, ep_len = env.reset(), 0, 0\n",
        "    for ep in range(max_ep):  \n",
        "        for t in range(max_steps):  \n",
        "            logit, action = sample_action(actor, torch.from_numpy(obs)) \n",
        "            obs_, reward, done, _ = env.step(action.detach().numpy())\n",
        "            ep_ret += reward\n",
        "    \n",
        "            buffer.store(\n",
        "                obs, \n",
        "                action.detach().numpy(), \n",
        "                reward / 10,\n",
        "                log_p(logit, action).detach().numpy(), \n",
        "                critic(torch.from_numpy(obs)).detach().numpy()\n",
        "            )\n",
        "            obs = obs_\n",
        "    \n",
        "            if done or (t == max_steps-1): \n",
        "                buffer.terminate(0 if done else critic(torch.from_numpy(obs)).detach().numpy())  \n",
        "    \n",
        "                print(f\"episode {ep}: return = {ep_ret}\")\n",
        "    \n",
        "                final_rewards.append(ep_ret)\n",
        "                obs, ep_ret = env.reset(), 0\n",
        "                break \n",
        "    \n",
        "        obs_buf, act_buf, adv_buf, return_buf, logp_buf = to_torch(*buffer.get())\n",
        "\n",
        "        policy_loss_fn(actor, policy_opt, obs_buf, act_buf, logp_buf, adv_buf)\n",
        "    \n",
        "        for j in range(value_iterations):\n",
        "            value_loss = ((critic(obs_buf)-return_buf)**2).mean()\n",
        "    \n",
        "            value_opt.zero_grad()\n",
        "            value_loss.backward()\n",
        "            value_opt.step()\n",
        "    \n",
        "    return np.mean(final_rewards)"
      ],
      "metadata": {
        "id": "aGgTExNU-IQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_custom, emp_ppo = [], []\n",
        "for k in range(10):\n",
        "    emp_custom.append(train_agent(custom_policy))\n",
        "    emp_ppo.append(train_agent(ppo_policy))"
      ],
      "metadata": {
        "id": "pg4FlmZPiXH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(emp_custom), np.std(emp_custom)"
      ],
      "metadata": {
        "id": "Rh_g5FjXx51j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(emp_ppo), np.std(emp_ppo)"
      ],
      "metadata": {
        "id": "Ro0Eszlc0kva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6hqxWZd_0mtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}