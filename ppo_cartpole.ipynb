{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyPbdPctSOize+nxA4j2VqlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asceznyk/notebooks/blob/main/ppo_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn21iArHKLqA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import scipy.signal as signal\n",
        "\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.distributions.categorical import Categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "dim_obs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "9V0GZj49KyU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## networks\n",
        "\n",
        "def mlp(layers, out_dim):\n",
        "    args = []\n",
        "    for u_in, u_out in layers:\n",
        "        args.append(nn.Linear(u_in, u_out))\n",
        "        args.append(nn.Tanh())\n",
        "    args.append((nn.Linear(u_out, out_dim)))\n",
        "    return nn.Sequential(*args)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, n_actions, hid_layers):\n",
        "        super(Actor, self).__init__()\n",
        "        self.net = mlp(hid_layers, n_actions) \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, hid_layers):\n",
        "        super(Critic, self).__init__()\n",
        "        self.net = mlp(hid_layers, 1) \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "UttAL6QWK7Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the buffer\n",
        "\n",
        "def discount_cum(x, d): \n",
        "    return signal.lfilter([1], [1, float(-d)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self, dim_obs, size, gamma=0.99, lam=0.95):\n",
        "        self.obs_buf = np.zeros((size, dim_obs), dtype=np.float32)\n",
        "        self.act_buf = np.zeros(size, dtype=np.int64)\n",
        "        self.adv_buf = np.zeros(size, dtype=np.float32) \n",
        "        self.return_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.reward_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.t_start = 0, 0\n",
        "\n",
        "    def terminate(self, last_val=0):\n",
        "        t_slice = slice(self.t_start, self.pointer)\n",
        "        rewards = np.append(self.reward_buf[t_slice], last_val)\n",
        "        values = np.append(self.value_buf[t_slice], last_val)\n",
        "        delta = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self.adv_buf[t_slice] = discount_cum(delta, self.gamma * self.lam)\n",
        "        self.return_buf[t_slice] = discount_cum(rewards, self.gamma)[:-1]\n",
        "        self.t_start = self.pointer\n",
        "\n",
        "    def store(self, obs, act, reward, logp, value):\n",
        "        self.obs_buf[self.pointer] = obs\n",
        "        self.act_buf[self.pointer] = act\n",
        "        self.reward_buf[self.pointer] = reward\n",
        "        self.logp_buf[self.pointer] = logp\n",
        "        self.value_buf[self.pointer] = value\n",
        "        self.pointer += 1\n",
        "\n",
        "    def get(self):\n",
        "        self.pointer, self.t_start = 0, 0 \n",
        "        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / \\\n",
        "                        np.std(self.adv_buf) \n",
        "        return (\n",
        "            self.obs_buf,\n",
        "            self.act_buf,\n",
        "            self.adv_buf,\n",
        "            self.return_buf,\n",
        "            self.logp_buf\n",
        "        )\n"
      ],
      "metadata": {
        "id": "90Gdjs3wQYXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## helper\n",
        "\n",
        "def to_torch(*args):\n",
        "    for t in args: \n",
        "        yield torch.from_numpy(t)\n",
        "\n",
        "def to_numpy(*args):\n",
        "    for t in args:\n",
        "        yield t.detach().numpy()\n",
        "\n",
        "def sample_action(actor, obs): \n",
        "    logits = actor(obs)\n",
        "    dist = Categorical(F.softmax(logits, dim=-1))\n",
        "    return logits, dist.sample()\n",
        "\n",
        "def log_p(logits, act_buf):\n",
        "    return torch.sum(\n",
        "        F.one_hot(act_buf, n_actions) * F.log_softmax(logits, dim=-1), dim=-1\n",
        "    )"
      ],
      "metadata": {
        "id": "VL355oPCmDbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## hyperparameters of the PPO algorithm\n",
        "\n",
        "steps_per_epoch = 4000\n",
        "epochs = 30\n",
        "gamma = 0.99\n",
        "eps = 0.2\n",
        "lr_policy = 3e-4\n",
        "lr_value = 1e-3\n",
        "policy_iterations = 80\n",
        "value_iterations = 80\n",
        "lam = 0.97\n",
        "target_kl = 0.01\n",
        "hidden_sizes = [(dim_obs, 64), (64, 64)]\n",
        "\n",
        "render = False\n"
      ],
      "metadata": {
        "id": "KXY1OFrfwj9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## inits\n",
        "\n",
        "actor = Actor(n_actions, hidden_sizes) \n",
        "critic = Critic(hidden_sizes)\n",
        "\n",
        "buffer = Buffer(dim_obs, steps_per_epoch)\n",
        "policy_opt = torch.optim.Adam(actor.parameters(), lr=lr_policy)\n",
        "value_opt = torch.optim.Adam(critic.parameters(), lr=lr_value)\n",
        "\n",
        "obs, ep_ret, ep_len = env.reset(), 0, 0"
      ],
      "metadata": {
        "id": "zDE-6FCSyvBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## typical training\n",
        "for e in range(epochs):\n",
        "    sum_return = 0\n",
        "    sum_len = 0\n",
        "    num_ep = 0\n",
        "\n",
        "    actor.eval()\n",
        "    critic.eval()\n",
        "    for t in range(steps_per_epoch):  \n",
        "        logit, action = sample_action(actor, torch.from_numpy(obs)) \n",
        "        obs_, reward, done, _ = env.step(action.detach().numpy())\n",
        "        ep_ret += reward\n",
        "        ep_len += 1\n",
        "\n",
        "        buffer.store(\n",
        "            obs, \n",
        "            action.detach().numpy(), \n",
        "            reward,\n",
        "            log_p(logit, action).detach().numpy(), \n",
        "            critic(torch.from_numpy(obs)).detach().numpy()\n",
        "        )\n",
        "        obs = obs_\n",
        "\n",
        "        if done or (t == steps_per_epoch-1): \n",
        "            buffer.terminate(0 if done else critic(torch.from_numpy(obs)).detach().numpy())\n",
        "            sum_return += ep_ret\n",
        "            sum_len += ep_len\n",
        "            num_ep += 1\n",
        "            obs, ep_ret, ep_len = env.reset(), 0, 0\n",
        " \n",
        "    obs_buf, act_buf, adv_buf, return_buf, logp_buf = to_torch(*buffer.get()) \n",
        "\n",
        "    actor.train()\n",
        "    critic.train()\n",
        "\n",
        "    for i in range(policy_iterations): \n",
        "        policy_loss = -torch.minimum(\n",
        "            (log_p(actor(obs_buf), act_buf) - logp_buf).exp() * adv_buf, \n",
        "            torch.where(adv_buf > 0, (1+eps)*adv_buf, (1-eps)*adv_buf)\n",
        "        ).mean() \n",
        "\n",
        "        policy_opt.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        policy_opt.step()\n",
        "        \n",
        "        kl = (logp_buf - log_p(actor(obs_buf), act_buf)).mean().sum() \n",
        "        if kl > 1.5 * target_kl: break\n",
        "\n",
        "    for j in range(value_iterations):\n",
        "        value_loss = ((critic(obs_buf)-return_buf)**2).mean()\n",
        "\n",
        "        value_opt.zero_grad()\n",
        "        value_loss.backward()\n",
        "        value_opt.step()\n",
        "\n",
        "    print(f\"Total episodes: {num_ep}\")\n",
        "    print(\n",
        "        f\"Epoch {e+1}: Mean return: {sum_return / num_ep}, Mean length: {sum_len / num_ep}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "aGgTExNU-IQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QopGHaT3RX0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}