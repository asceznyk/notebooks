{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/commaai/calib_challenge.git ","metadata":{"_uuid":"f548f3c4-8793-47d7-9280-8cc5426be548","_cell_guid":"9c1a7e46-aefc-414c-9976-79621fca050e","execution":{"iopub.status.busy":"2022-07-24T06:31:41.791808Z","iopub.execute_input":"2022-07-24T06:31:41.792510Z","iopub.status.idle":"2022-07-24T06:32:26.536749Z","shell.execute_reply.started":"2022-07-24T06:31:41.792473Z","shell.execute_reply":"2022-07-24T06:32:26.535802Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/asceznyk/calipy.git","metadata":{"_uuid":"4c38b79e-d7ff-4dfe-9289-7ee9278c5c9a","_cell_guid":"a2c72b52-b4d0-4361-8abb-ad0a7a64e777","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repo_path = '/kaggle/working/calipy/'\nchallenge_path = '/kaggle/working/calib_challenge/'\ntrain_path = challenge_path+'labeled/'\ntest_path = challenge_path+'unlabeled/'\npredictions_path = challenge_path+'predictions/'\n\nweights_path = '/kaggle/input/calibw/calibnet.best'","metadata":{"_uuid":"088cd20c-ac57-4c0d-a0f6-e6c1d53c4139","_cell_guid":"9c220795-02c6-4ffa-9e1c-47df6fa94c92","execution":{"iopub.status.busy":"2022-07-24T06:35:10.341297Z","iopub.execute_input":"2022-07-24T06:35:10.341593Z","iopub.status.idle":"2022-07-24T06:35:10.347786Z","shell.execute_reply.started":"2022-07-24T06:35:10.341556Z","shell.execute_reply":"2022-07-24T06:35:10.347005Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport cv2\nimport random\nimport math\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.utils as VU\nimport torchvision.transforms.functional as VF\nimport torchvision.transforms as T\nimport torchvision.models as models\n\nfrom torch.utils.data import Dataset, DataLoader\n\nplt.rcParams[\"figure.figsize\"] = (20, 10)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:32:26.546351Z","iopub.execute_input":"2022-07-24T06:32:26.546657Z","iopub.status.idle":"2022-07-24T06:32:29.295339Z","shell.execute_reply.started":"2022-07-24T06:32:26.546620Z","shell.execute_reply":"2022-07-24T06:32:29.294270Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def display_image_in_actual_size(img_data):\n    dpi = 80\n    height, width, depth = img_data.shape\n    figsize = width / float(dpi), height / float(dpi)\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.axis('off')\n    ax.imshow(img_data, cmap='gray')\n    plt.show()\n\ndef display_image_label_pairs(video_path, label_path, num_frames=1, idx_frame=0):\n    cap = cv2.VideoCapture(video_path)\n    ret = True\n    i = 0\n    \n    label = np.loadtxt(label_path)\n    lp = np.nan_to_num(label[:, 0])\n    ly = np.nan_to_num(label[:, 1])\n    \n    while ret and i < label.shape[0]:\n        ret, img = cap.read() \n        if ret:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if i in range(idx_frame, idx_frame+num_frames):\n            display_image_in_actual_size(img)\n            print(label[i])\n        i += 1\n        \n    print(f\"mean for pitch and yaw {label_path}:{np.mean(lp):.3f}, {np.mean(ly):.3f}\")\n    print(f\"std for pitch and yaw {label_path}:{np.std(lp):.3f}, {np.std(ly):.3f}\")\n    print(\"-\"*40)\n\ndef save_frames(videos_dir, resize=0):\n    for video_path in [file for file in os.listdir(videos_dir) if file.endswith('.hevc')]: \n        video_path = f'{videos_dir}/{video_path}'\n        cap = cv2.VideoCapture(video_path)\n        ret = True\n        f = 1\n        while ret:\n            ret, img = cap.read()\n            if ret:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                if resize:\n                    img = cv2.resize(img, dsize=(img_size[2], img_size[1]), interpolation=cv2.INTER_AREA)\n                cv2.imwrite(f\"{video_path.replace('.hevc', '')}_{f}.jpg\", img)\n            f += 1","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:32:29.297938Z","iopub.execute_input":"2022-07-24T06:32:29.298219Z","iopub.status.idle":"2022-07-24T06:32:29.312666Z","shell.execute_reply.started":"2022-07-24T06:32:29.298166Z","shell.execute_reply":"2022-07-24T06:32:29.311483Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def display_video_cap_text(video_path, label_path):\n    cap = cv2.VideoCapture(video_path)\n    ret = True\n    i = 0\n    \n    label = np.loadtxt(label_path)\n    lp = np.nan_to_num(label[:, 0])\n    ly = np.nan_to_num(label[:, 1])\n    \n    while ret and i < label.shape[0]:\n        ret, img = cap.read() \n        if ret:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            text = label[i].astype(str)\n            cv2.putText(img, str(text), (10, 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,0,0), 1, cv2.LINE_AA)\n            \n        plt.imshow(img)\n        plt.show()\n        i += 1\n        \n    plt.close('all')\n        ","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:32:29.313910Z","iopub.execute_input":"2022-07-24T06:32:29.314347Z","iopub.status.idle":"2022-07-24T06:32:29.327060Z","shell.execute_reply.started":"2022-07-24T06:32:29.314289Z","shell.execute_reply":"2022-07-24T06:32:29.326083Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"save_frames(f'{challenge_path}labeled')","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:32:29.329702Z","iopub.execute_input":"2022-07-24T06:32:29.330344Z","iopub.status.idle":"2022-07-24T06:35:10.210719Z","shell.execute_reply.started":"2022-07-24T06:32:29.330305Z","shell.execute_reply":"2022-07-24T06:35:10.209843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"max_scale = 1 #* 180/math.pi\nimg_size = (3, 188, 250)\nlabel_size = 2\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.212192Z","iopub.execute_input":"2022-07-24T06:35:10.212490Z","iopub.status.idle":"2022-07-24T06:35:10.269343Z","shell.execute_reply.started":"2022-07-24T06:35:10.212452Z","shell.execute_reply":"2022-07-24T06:35:10.268293Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = VF.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.271105Z","iopub.execute_input":"2022-07-24T06:35:10.272146Z","iopub.status.idle":"2022-07-24T06:35:10.279303Z","shell.execute_reply.started":"2022-07-24T06:35:10.272104Z","shell.execute_reply":"2022-07-24T06:35:10.278393Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CalibData(Dataset):\n    def __init__(self, img_paths, labels, transform=None):\n        self.img_paths = img_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, i):\n        img = VF.resize(VF.to_tensor(Image.open(self.img_paths[i])), (img_size[1], img_size[2]))\n        label = torch.from_numpy(self.labels[i] * max_scale).float()\n        \n        if self.transform is not None:\n            img = self.transform(img)\n        return img, label\n    \nclass DummyData(Dataset):\n    def __init__(self, img_size, labels):\n        self.mats = torch.zeros((labels.shape[0], *img_size))\n        self.labels = labels\n\n    def __len__(self):\n        return self.mats.shape[0]\n\n    def __getitem__(self, i):\n        mat = self.mats[i]\n        label = torch.from_numpy(self.labels[i] * max_scale).float()\n        return mat, label\n    \ndef view_angle_images(data, start, end):\n    show([data[i][0] for i in range(start, end)])\n    print([data[i][1] for i in range(start, end)])\n    \ndef load_img_path_labels(input_dir):\n    labels = []\n    img_paths = []\n    for file in sorted([f for f in os.listdir(input_dir) if f.endswith('txt')]):\n        labels.append(np.loadtxt(f'{input_dir}/{file}'))\n        for l in range(1, len(labels[-1])+1):\n            img_paths.append(f\"{input_dir}/{file.replace('.txt', '')}_{l}.jpg\")\n    return np.array(img_paths), np.vstack(labels)\n\ndef get_mse(gt, test):\n    test = np.nan_to_num(test)\n    return np.mean(np.nanmean((gt - test)**2, axis=0))\n\ndef to_radians(deg):\n    return deg * math.pi / 180\n\ndef mse_zero_percent(gt, mp, convert=0):\n    if convert:\n        gt = to_radians(gt)\n        mp = to_radians(mp)\n        \n    err_mse = get_mse(gt, mp)\n    zero_mse = get_mse(gt, np.zeros_like(gt))\n    \n    return 100 * (err_mse / (zero_mse if zero_mse > 0 else 1.25e-3))","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.280842Z","iopub.execute_input":"2022-07-24T06:35:10.281153Z","iopub.status.idle":"2022-07-24T06:35:10.298662Z","shell.execute_reply.started":"2022-07-24T06:35:10.281116Z","shell.execute_reply":"2022-07-24T06:35:10.297709Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def fill_zeros_previous(arr):\n    for i, r in enumerate(arr):\n        if r.sum() == 0 and i > 0:\n            arr[i] = arr[i-1]\n    return arr\n            \ndef remove_zero_labels(x, y):\n    y = y[np.all(y != 0, axis=1)]\n    x = x[np.where(np.any(y != 0, axis=1))[0]]\n    return x, y\n            \ndef split_data(img_paths, labels, split=0.90, transform=None, non_zero_labels=1, remove_nans=1):\n    labels = np.nan_to_num(labels)\n    \n    if non_zero_labels:\n        if remove_nans:\n            img_paths, labels = remove_zero_labels(img_paths, labels)\n        else:\n            labels = fill_zeros_previous(labels)\n \n    x_train, x_test, y_train, y_test = train_test_split(img_paths, labels, test_size=(1.0 - split), random_state=42)\n    train_size = int(split * x_train.shape[0])\n    x_valid, y_valid, x_train, y_train = x_train[train_size:], y_train[train_size:], x_train[:train_size], y_train[:train_size]\n\n    train_data = CalibData(x_train, y_train, transform=transform)\n    valid_data = CalibData(x_valid, y_valid)\n    test_data = CalibData(x_test, y_test)\n    \n    return train_data, valid_data, test_data\n\ndef load_pretrained_model(model, weights_path):\n    model.load_state_dict(torch.load(weights_path))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.302400Z","iopub.execute_input":"2022-07-24T06:35:10.302701Z","iopub.status.idle":"2022-07-24T06:35:10.315989Z","shell.execute_reply.started":"2022-07-24T06:35:10.302664Z","shell.execute_reply":"2022-07-24T06:35:10.315087Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, c_in, c_out, stride=1, leaky=1):\n        super(ResBlock, self).__init__()\n        self.skip = None\n        self.leaky = leaky\n        \n        if stride != 1 or c_in != c_out:\n            self.skip = nn.Sequential(\n                nn.Conv2d(c_in, c_out, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(c_out)\n            )\n\n        self.block = nn.Sequential(\n            nn.Conv2d(c_in, c_out, 3, padding=1, stride=1, bias=False),\n            nn.BatchNorm2d(c_out),\n            nn.LeakyReLU() if leaky else nn.ReLU(inplace=True),\n            nn.Conv2d(c_in, c_out, 3, padding=1, stride=1, bias=False),\n            nn.BatchNorm2d(c_out)\n        )\n        \n    def forward(self, x):\n        act = F.leaky_relu if self.leaky else F.relu \n        return act(self.block(x) + x if self.skip is None else self.skip(x))\n\n    \nclass CalibNet(nn.Module):\n    def __init__(self, img_dim, ang_dim, act='relu'):\n        super(CalibNet, self).__init__()\n        self.img_dim = img_dim\n        self.ang_dim = ang_dim\n        \n        self.base_cnn = nn.Sequential(\n            torch.nn.Sequential(*(list(models.resnet18().children())[:-1])),\n\n            #self.cnn_block(img_dim[0], 24, 5, 2),\n            #self.cnn_block(24, 36, 5, 2),\n            #self.cnn_block(36, 48, 5, 2),\n            #self.cnn_block(48, 60, 5, 2),\n            #self.cnn_block(60, 72, 5, 2),\n            #self.cnn_block(72, 84, 3, 2),\n            #self.cnn_block(84, 96, 3, 2)\n        )\n        \n        self.base_dense = nn.Sequential(\n            nn.Flatten(),\n            #self.linear_block(512, 100),\n            #self.linear_block(100, 50),\n            #self.linear_block(50, 10),\n            nn.Linear(512, ang_dim)\n        )\n        \n        \n        self.act = act\n        '''self.d_acts = {\n            'leaky': nn.LeakyReLU(),\n            'relu': nn.ReLU(inplace=True)\n        }'''\n        \n        self.base_dense[-1].bias.data = torch.tensor([0.0277611 * max_scale , 0.02836007 * max_scale])\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.kaiming_uniform_(m.weight)\n            \n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        \n    def cnn_block(self, c_in, c_out, k_size, stride, pad=0, bias=False):\n        return nn.Sequential(\n            nn.Conv2d(c_in, c_out, k_size, stride, padding=pad, bias=bias),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(c_out),\n        )\n\n    def linear_block(self, in_units, out_units, bias=True):\n        return nn.Sequential(\n            nn.Linear(in_units, out_units, bias=bias),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x, y=None):\n        x = self.base_cnn(x)\n        p = self.base_dense(x)\n\n        loss = None\n        if y is not None: \n            loss = F.mse_loss(p, torch.nan_to_num(y))\n\n        return p, loss","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.319549Z","iopub.execute_input":"2022-07-24T06:35:10.319782Z","iopub.status.idle":"2022-07-24T06:35:10.339971Z","shell.execute_reply.started":"2022-07-24T06:35:10.319748Z","shell.execute_reply":"2022-07-24T06:35:10.339228Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def visualize_feature_maps(model, x):\n    model.eval()\n    \n    conv_layers = []\n    model_children = list(model.base_cnn.children())\n\n    def append_conv(mc_l):\n        for child in mc_l:\n            conv_layers.append(child)\n                \n    def process(x):\n        feature_map = x.squeeze(0)\n        gray_scale = torch.sum(feature_map, 0)\n        gray_scale = gray_scale / feature_map.shape[0]\n        return gray_scale.data.cpu().numpy()\n    \n    for i in range(len(model_children)):\n        if type(model_children[i]) == nn.Sequential:\n            append_conv(model_children[i].children())\n        elif type(model_children[i]) == ResBlock:\n            append_conv(model_children[i].block.children())\n        else:\n            conv_layers.append(model_children[i])\n     \n    outputs = []\n    names = []\n    \n    for layer in conv_layers[0:]:\n        x = layer(x)\n        outputs.append(process(x))\n        names.append(str(layer))\n    \n    print(len(outputs))\n    \n    for feature_map in outputs:\n        print(feature_map.shape)\n        \n    fig = plt.figure(figsize=(20, 20))\n    min_len = int(np.sqrt(len(outputs)) + 1)\n    for i in range(len(outputs)):\n        a = fig.add_subplot(min_len, min_len, i+1)\n        imgplot = plt.imshow(np.abs(outputs[i]), cmap='gray', norm=plt.Normalize(0, 1))\n        a.axis(\"off\")\n        a.set_title(f\"{names[i].split('(')[0]} -- {i}\", fontsize=10)\n        \n    plt.show()\n    \n    for o in outputs:\n        print(np.abs(o))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.349265Z","iopub.execute_input":"2022-07-24T06:35:10.349609Z","iopub.status.idle":"2022-07-24T06:35:10.363382Z","shell.execute_reply.started":"2022-07-24T06:35:10.349574Z","shell.execute_reply":"2022-07-24T06:35:10.362684Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"output_int = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        output_int[name] = output.detach()\n    return hook","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.366158Z","iopub.execute_input":"2022-07-24T06:35:10.366452Z","iopub.status.idle":"2022-07-24T06:35:10.373974Z","shell.execute_reply.started":"2022-07-24T06:35:10.366409Z","shell.execute_reply":"2022-07-24T06:35:10.373198Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"img_paths, labels = load_img_path_labels(f'{challenge_path}labeled')\ntrain_data, valid_data, test_data = split_data(img_paths, labels, transform=None)\nmodel = CalibNet(img_size, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data), len(test_data), len(valid_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[2][0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.close('all')\nvisualize_feature_maps(model, train_data[0][0].unsqueeze(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(model, optimizer, train_loader, valid_loader=None, ckpt_path=None, epochs=10, lr=0.001, log_preds=[0,1]): \n    print(f'the learning rate chosen: {lr}')\n    \n    if type(log_preds) == int:\n        log_preds = [log_preds, log_preds]\n\n    def run_epoch(split, log=0):\n        is_train = split == 'train' \n        model.train(is_train)\n        loader = train_loader if is_train else valid_loader\n\n        avg_loss = 0\n        avg_mse_percent = 0\n        pbar = tqdm(enumerate(loader), total=len(loader))\n        for step, batch in pbar:\n            batch = [i.to(device) for i in batch]\n            imgs, labels = batch\n            \n            with torch.set_grad_enabled(is_train):\n                preds, loss = model(imgs, labels)\n                avg_loss += loss.item() / len(loader)\n                avg_mse_percent += mse_zero_percent(np.nan_to_num(labels.detach().cpu().numpy()),  preds.detach().cpu().numpy()) / len(loader)\n                \n                if log:\n                    print('-'*40)\n                    print(f'predictions for frames ->')\n                    print(preds)\n                    print(f'labels for frames ->')\n                    print(torch.nan_to_num(labels))\n                    print('-'*40)\n\n            if is_train:\n                model.zero_grad() \n                loss.backward() \n                optimizer.step()\n\n            pbar.set_description(f\"epoch: {e+1}, avg_loss: {avg_loss:.6f}, avg_mse_percent: {avg_mse_percent:.3f}%\") \n\n        return avg_loss\n\n    model.to(device)\n\n    best_loss = float('inf') \n    train_losses, valid_losses = [], []\n    for e in range(epochs):\n        train_loss = run_epoch('train', log_preds[0])\n        valid_loss = run_epoch('valid', log_preds[1]) if valid_loader is not None else train_loss\n        \n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        \n        if ckpt_path is not None and valid_loss < best_loss:\n            best_loss = valid_loss\n            torch.save(model.state_dict(), ckpt_path)\n            \n    return train_losses, valid_losses","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.375411Z","iopub.execute_input":"2022-07-24T06:35:10.375767Z","iopub.status.idle":"2022-07-24T06:35:10.391165Z","shell.execute_reply.started":"2022-07-24T06:35:10.375729Z","shell.execute_reply":"2022-07-24T06:35:10.390417Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataset, model, log_preds=1, batch_size=4, non_zero_labels=1, convert=0):\n    model.to(device)\n    model.eval()\n    \n    loader = DataLoader(dataset, batch_size=batch_size)\n    avg_mse_percent = 0\n    for batch in loader:\n        batch = [i.to(device) for i in batch]\n        imgs, labels = batch\n\n        with torch.no_grad(): \n            preds, loss = model(imgs, labels)\n            preds, labels = preds.detach().cpu().numpy(), labels.detach().cpu().numpy()\n            \n            if non_zero_labels:\n                preds, labels = remove_zero_labels(preds, labels)\n            \n            avg_mse_percent += mse_zero_percent(preds, labels, convert=convert) / len(loader)\n\n            if log_preds:\n                print('-'*40)\n                print(f'predictions for frame ->')\n                print(preds)\n                print(f'labels for farme ->')\n                print(labels)\n                print('-'*40)\n                \n    print(f'mse error wrt zero error: {avg_mse_percent:.3f}%')","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.392492Z","iopub.execute_input":"2022-07-24T06:35:10.392775Z","iopub.status.idle":"2022-07-24T06:35:10.403716Z","shell.execute_reply.started":"2022-07-24T06:35:10.392742Z","shell.execute_reply":"2022-07-24T06:35:10.402983Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def main(main_dir, epochs=50, batch_size=4, learning_rate=5e-3, single_batch=0, zero_input=0, pretrained_weights=''):\n    torch.manual_seed(0)\n\n    img_paths, labels = load_img_path_labels(f'{challenge_path}labeled')\n    train_data, valid_data, test_data = split_data(img_paths, labels)\n    labels = np.nan_to_num(labels)\n\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_data, batch_size=batch_size)\n    test_loader = DataLoader(test_data, batch_size=batch_size)\n\n    model = CalibNet(img_size, label_size)\n    \n    print(model)\n    print(f'total number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n    \n    if pretrained_weights != '':\n        print(f'loading pretrained model from {pretrained_weights}.. ')\n        load_pretrained_model(model, pretrained_weights)\n        \n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n\n    if single_batch:\n        random_idx = 1200\n        labels = fill_zeros_previous(labels)\n        if not zero_input:\n            single_data = CalibData(img_paths[random_idx:random_idx+batch_size], labels[random_idx:random_idx+batch_size])\n            #view_angle_images(single_data, 0, batch_size)\n            single_batch = DataLoader(single_data, batch_size=batch_size)\n        else:\n            print(f'all inputs to model are zeros, checking training results..')\n            single_batch = DataLoader(DummyData(img_size, labels[random_idx: random_idx+batch_size]), batch_size=batch_size)\n\n        train_losses, valid_losses = fit(model, optimizer, single_batch, epochs=epochs, lr=learning_rate, log_preds=1)\n        #visualize_intermediate_maps(model, single_data[0][0].unsqueeze(0).to(device))\n    else:\n        train_losses, valid_losses = fit(model, optimizer, train_loader, valid_loader, epochs=epochs, lr=learning_rate, log_preds=0, ckpt_path='calibnet.best')\n        plt.plot(valid_losses)\n        \n    plt.plot(train_losses)\n    plt.show()\n    \n    return model, train_data, valid_data, test_data","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.405184Z","iopub.execute_input":"2022-07-24T06:35:10.405578Z","iopub.status.idle":"2022-07-24T06:35:10.418414Z","shell.execute_reply.started":"2022-07-24T06:35:10.405544Z","shell.execute_reply":"2022-07-24T06:35:10.417403Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\nmodel, _, _, _ = main(train_path, epochs=200, batch_size=4, learning_rate=1e-4, single_batch=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:35:10.420020Z","iopub.execute_input":"2022-07-24T06:35:10.420318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel, train_data, valid_data, test_data = main(train_path, epochs=10, batch_size=4, learning_rate=1e-4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CalibNet(img_size, 2)\nmodel.load_state_dict(torch.load('calibnet.best'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.base_dense[3].register_forward_hook(get_activation('fc3'))\nmodel(train_data[2][0].unsqueeze(0).to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_int","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.close('all')\nvisualize_feature_maps(model, valid_data[60][0].unsqueeze(0).to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(train_data, model, log_preds=1, non_zero_labels=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(test_data, model, log_preds=1, non_zero_labels=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#labels = np.nan_to_num(labels)\n#labels[np.all(labels != 0, axis=1)]\n#img_paths[np.where(np.any(labels != 0, axis=1))[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('calibnet.best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}